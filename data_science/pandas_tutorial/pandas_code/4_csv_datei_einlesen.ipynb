{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50413ef1",
   "metadata": {},
   "source": [
    "# CSV-Datei in einen DataFrame einlesen\n",
    "Pandas bietet mit der Methode `read_csv()` eine einfache Methode, eine CSV-Datei einzulesen. Die Methode bietet unzählige Parameter. Mehr dazu in der Doku: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2fdbbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144038c",
   "metadata": {},
   "source": [
    "## Beispiel: Firmenmitarbeiter\n",
    "Unsere Beispiel-Datei ist eine fiktive Mitarbeiter-Datei mit 500 Mitarbeitern und 11 Spalten.\n",
    "Wir wollen die Datei einlesen und untersuchen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc915d",
   "metadata": {},
   "source": [
    "### 1. Datei mit read_csv einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d53c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv datei öffnen und DataFrame erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60e215",
   "metadata": {},
   "source": [
    "### 2. die ersten 5 Zeilen ausgeben lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a43aede",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01397b2",
   "metadata": {},
   "source": [
    "### 3. die letzte Zeile ausgeben lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33335965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>company_name</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "      <th>postal</th>\n",
       "      <th>phone1</th>\n",
       "      <th>phone2</th>\n",
       "      <th>email</th>\n",
       "      <th>web</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Mi</td>\n",
       "      <td>Richan</td>\n",
       "      <td>Nelson Wright Haworth Golf Crs</td>\n",
       "      <td>6 Norwood Grove</td>\n",
       "      <td>Tanworth-in-Arden</td>\n",
       "      <td>Warwickshire</td>\n",
       "      <td>B94 5RZ</td>\n",
       "      <td>01451-785624</td>\n",
       "      <td>01202-738406</td>\n",
       "      <td>mi@hotmail.com</td>\n",
       "      <td>http://www.nelsonwrighthaworthgolfcrs.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    first_name last_name                    company_name          address  \\\n",
       "499         Mi    Richan  Nelson Wright Haworth Golf Crs  6 Norwood Grove   \n",
       "\n",
       "                  city        county   postal        phone1        phone2  \\\n",
       "499  Tanworth-in-Arden  Warwickshire  B94 5RZ  01451-785624  01202-738406   \n",
       "\n",
       "              email                                          web  \n",
       "499  mi@hotmail.com  http://www.nelsonwrighthaworthgolfcrs.co.uk  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138fa492",
   "metadata": {},
   "source": [
    "### die Spaltennamen ausgeben lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6945a284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['first_name', 'last_name', 'company_name', 'address', 'city', 'county',\n",
       "       'postal', 'phone1', 'phone2', 'email', 'web'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165e2de5",
   "metadata": {},
   "source": [
    "### Shape und Dimensionen\n",
    "Anhand des shapes kann man sehen, wie groß die Datei ist. Der erste Wert bildet die Zeilen des Dataframes ab, der zweite Wert repräsentiert die Spalten. Die Dimension ist 2, wie das bei einem Dataframe immer der Fall ist, da es sich ja um eine Tabelle handelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3536a553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 11), 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdab14e",
   "metadata": {},
   "source": [
    "### eindeutige Werte einer Spalte (unique)\n",
    "Oft ist es von Interesse, wieviele eindeutige Werte eine Spalte hat. Wir interessieren uns für die eindeutigen Werte in der Spalte `county`. Dafür können wir die Methode `unique()` auf eine Series (Spalte) anwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c687a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_counties = df.county.unique()\n",
    "unique_counties.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ae2f1",
   "metadata": {},
   "source": [
    "## Spalten beim Einlesen der Datei auslassen\n",
    "Falls wir nicht an allen Spalten interessiert sind, können wir sie auch gleich beim Einlesen auslassen. Wir interessieren uns jetzt nur für die Spalten `first_name`, `last_name` und `company_name`. Diese Technik verringert maßgeblich den Speicherverbrauch im RAM und sollte bei sehr großen Dateien durchgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d4ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data/uk-500.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cd023eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   first_name    500 non-null    object\n",
      " 1   last_name     500 non-null    object\n",
      " 2   company_name  500 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_employees.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4776a6-9f20-43fb-a329-6261e5a66fdb",
   "metadata": {},
   "source": [
    "## Datentyp beim Auslesen angeben\n",
    "\n",
    "Eine weitere Methoden, den Speicherverbrauch zu reduzieren, ist es, die Datentypen der Spalten beim Laden der CSV-Datei anzugeben. Somit kann der Speicherverbrauch vor allem bei vorrangig numerischen Daten maßgeblich reduziert werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "662ae3df-2d1b-4c7b-ab22-6e43c2f3d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data/penguin_size.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd881af7-bb1f-4515-8907-a8728c00be08",
   "metadata": {},
   "source": [
    "## Daten in Teilen einlesen\n",
    "\n",
    "Bei sehr großen CSV-Dateien kann es sinnvoll sein, die Daten in Teilen zu lesen, um den Arbeitsspeicher zu schonen. Dies ist besonders nützlich, wenn die Datei zu groß ist, um sie komplett in den Speicher zu laden. Eine empfohlene maximale Größe für die Arbeit mit Pandas liegt bei 5 bis 10 Millionen Zeilen oder etwa 1 bis 5 GB Daten, je nach Komplexität der Operationen und verfügbaren Arbeitsspeicher des Systems. Alternativ zum Batch-Prozess kann auch eine andere Libary gewählt werden: siehe Dask, Pyspark, Vaex und andere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94867307-9e68-456f-ac1b-dbd70921b14c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Daten stückweise einlesen um sehr große Datei einzulesen und zu filtern. \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create an empty DataFrame to store the filtered results\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read the CSV file in chunks\u001b[39;00m\n\u001b[1;32m      7\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Adjust the chunk size as needed\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Daten stückweise einlesen um sehr große Datei einzulesen und zu filtern. \n",
    "\n",
    "# Create an empty DataFrame to store the filtered results\n",
    "filtered_df = pd.DataFrame()\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "chunk_size = 1000  # Adjust the chunk size as needed\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546de93-72d9-40ce-8f7d-018b14287fa1",
   "metadata": {},
   "source": [
    "## Ergebnis als Pickle speichern\n",
    "\n",
    "Du kannst das gefilterte DataFrame als komprimierte Pickle-Datei speichern, um Speicherplatz zu sparen und die Ladezeiten zu verbessern. Hinweis: Pickle-Dateien sind nur im Python-Kosmus gebräuchlich (siehe parquet-Format für Alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90bb896-d20f-402b-802c-156b0c52e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"data/filtered_penguins.pkl.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca442c-17e2-432a-a0e0-12b54055622d",
   "metadata": {},
   "source": [
    "## Pickle laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6b3a2b5-9793-4ef9-b2b5-d874dd6f4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle Datei laden und in DataFrame umforen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15028d29-6c09-4682-8426-47e8150a006d",
   "metadata": {},
   "source": [
    "## Parquet-Format\n",
    "\n",
    "Eine sprachagnostische Alternative zu Pickle ist das Parquet-Format, das von vielen Programmiersprachen und Datenverarbeitungstools unterstützt wird, wie z.B. Python, R, Java, Apache Spark, etc. Parquet ist ein spaltenbasiertes Speicherformat, das effizient in der Speicherung und Abfrage von großen Datenmengen ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43eb8f53-7096-47bb-a7c1-4c97c4ca1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Angenommen, du hast ein gefiltertes DataFrame 'filtered_df'\n",
    "# Speichere das DataFrame im Parquet-Format mit Kompression\n",
    "\n",
    "# Lade die gespeicherte Parquet-Datei\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
